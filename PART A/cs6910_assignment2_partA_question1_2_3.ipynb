{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mak109/cs6910_assignment2/blob/main/PART%20A/cs6910_assignment2_partA_question1_2_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training CNN from Scratch"
      ],
      "metadata": {
        "id": "mdhEiMuqlUwE"
      },
      "id": "mdhEiMuqlUwE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b4cbe4a",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "!pip install wget"
      ],
      "id": "8b4cbe4a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57958201",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# !pip install wandb -qqq\n",
        "# import wandb\n",
        "# wandb.login()\n",
        "# from wandb.keras import WandbCallback"
      ],
      "id": "57958201"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c739a581"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import wget\n",
        "import datetime\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "from PIL import Image\n",
        "plt.rcParams[\"figure.figsize\"] = (20,10)"
      ],
      "id": "c739a581"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe89a953"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers,Sequential,regularizers,optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import random\n",
        "random.seed(123)"
      ],
      "id": "fe89a953"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b44f191",
        "outputId": "4f26ac19-0cbe-4ba3-b549-615ed1fa2c5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting all the files now...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "url='https://storage.googleapis.com/wandb_datasets/nature_12K.zip'\n",
        "filename = os.path.basename(url)\n",
        "if not os.path.exists(filename) and not os.path.exists(\"inaturalist_12K\"):\n",
        "  filename = wget.download(url)\n",
        "  with ZipFile(filename, 'r') as z:\n",
        "    print('Extracting all the files now...')\n",
        "    z.extractall()\n",
        "    print('Done!')\n",
        "  os.remove(filename)"
      ],
      "id": "7b44f191"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa298523"
      },
      "outputs": [],
      "source": [
        "image_size = (256,256)\n",
        "num_layers = 5\n",
        "num_classes = 10"
      ],
      "id": "fa298523"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb1b05f7"
      },
      "outputs": [],
      "source": [
        "def CNN(config):\n",
        "    model = Sequential([\n",
        "        layers.Input((image_size[0],image_size[1],3)),\n",
        "        layers.Rescaling(1./255)\n",
        "        ])\n",
        "    \n",
        "    for l in range(num_layers):\n",
        "        model.add(layers.Conv2D(filters=config[\"filters_list\"][l],kernel_size=(config[\"kernel_sizes\"][l][0],config[\"kernel_sizes\"][l][1]),\n",
        "                        activation=config[\"activation\"],padding=\"same\",kernel_regularizer=regularizers.l2(config[\"weight_decay\"])))\n",
        "        if config[\"batch_normalization\"] == 'True':\n",
        "            model.add(layers.BatchNormalization())\n",
        "        model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(config[\"dense_layer_size\"],activation=config[\"activation\"],kernel_regularizer=regularizers.l2(config[\"weight_decay\"])))\n",
        "    model.add(layers.Dropout(config[\"dropout\"]))\n",
        "\n",
        "    model.add(layers.Dense(num_classes,activation=\"softmax\"))\n",
        "    return model"
      ],
      "id": "fb1b05f7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "537a926a"
      },
      "outputs": [],
      "source": [
        "#Training goes here\n",
        "\n",
        "def train(config_in=None,checkpointing=False):\n",
        "    #default configuration\n",
        "    config_ = {\n",
        "    \"kernel_sizes\" : [(3,3),(3,3),(5,5),(3,3),(3,3)],\n",
        "    \"activation\" : 'relu',\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"filters_list\" : [32,32,64,64,128],\n",
        "    \"dense_layer_size\" : 128,\n",
        "    \"batch_normalization\": \"True\",\n",
        "    \"data_augment\": \"False\",\n",
        "    \"weight_decay\":0.0005,\n",
        "    \"dropout\":0.2,\n",
        "    \"batch_size\":64,\n",
        "    \"epochs\":3\n",
        "    }\n",
        "    '''Wandb Configs'''\n",
        "    # wandb.init(config=config_)\n",
        "    # config = wandb.config\n",
        "    #Setting run name for better readability\n",
        "    # wandb.run.name = \"nd_\"+str(config[\"dense_layer_size\"])+\"bs_\"+str(config[\"batch_size\"])+\"ac_\"+str(config[\"activation\"])\n",
        "    #Some data preprocessing and train,val splitting\n",
        "    if config_in is not None:\n",
        "          config = config_in\n",
        "    else:\n",
        "          config = config_ #Default Config\n",
        "\n",
        "    val_generator = ImageDataGenerator(dtype=tf.float32,validation_split=0.1,data_format='channels_last').flow_from_directory(\n",
        "        'inaturalist_12K/train',\n",
        "        target_size = image_size,\n",
        "        batch_size = config['batch_size'],\n",
        "        color_mode = 'rgb',\n",
        "        class_mode = 'sparse',\n",
        "        shuffle=True,\n",
        "        subset='validation',\n",
        "        seed=123\n",
        "    \n",
        "    )\n",
        "    #Data Augmentation\n",
        "    if config[\"data_augment\"] == 'True':\n",
        "        data_generator = ImageDataGenerator(\n",
        "        rotation_range=50, #random rotation between -50(clockwise) to 50(anti-clockwise) degree\n",
        "        brightness_range=(0.2,0.8), \n",
        "        zoom_range=0.3, #zoom in range from [0.7,1.3]\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "        width_shift_range=0.1, #Horizontal Shifting as a ratio of width\n",
        "        height_shift_range=0.2,#Vertical Shifting as a ratio of height\n",
        "        data_format='channels_last',\n",
        "        validation_split=0.1,\n",
        "        dtype=tf.float32\n",
        "        )\n",
        "    else:\n",
        "        data_generator = ImageDataGenerator(\n",
        "            data_format='channels_last',\n",
        "            validation_split=0.1,\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "    #Train set creation after conditional augmentation\n",
        "    train_generator = data_generator.flow_from_directory(\n",
        "    'inaturalist_12K/train',\n",
        "    target_size = image_size,\n",
        "    batch_size = config['batch_size'],\n",
        "    color_mode = 'rgb',\n",
        "    class_mode = 'sparse',\n",
        "    shuffle=True,\n",
        "    subset='training',\n",
        "    seed=123\n",
        "    )\n",
        "    #Building Model based on config \n",
        "    model = CNN(config)\n",
        "    \n",
        "    #Compiling model \n",
        "    model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=config[\"learning_rate\"]),\n",
        "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        "    )\n",
        "    #For checkpointing default value is False\n",
        "    if checkpointing == True:\n",
        "        current_directory = os.getcwd()\n",
        "        final_directory = os.path.join(current_directory, f'models_{datetime.datetime.now()}')\n",
        "        if not os.path.exists(final_directory):\n",
        "            os.makedirs(final_directory)\n",
        "        checkpoint_filepath = final_directory\n",
        "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "          filepath=checkpoint_filepath,\n",
        "          save_weights_only=False,\n",
        "          monitor='val_accuracy',\n",
        "          mode='max',\n",
        "          save_best_only=True)\n",
        "          #Fitting Model\n",
        "        history = model.fit(train_generator,\n",
        "          validation_data=val_generator,\n",
        "          epochs=config[\"epochs\"],\n",
        "          verbose=1,\n",
        "          # callbacks = [WandbCallback()] #Used with wandb\n",
        "          callbacks = [model_checkpoint_callback] #Custom callback for checkpointing\n",
        "          )\n",
        "    else:\n",
        "        history = model.fit(train_generator,\n",
        "          validation_data=val_generator,\n",
        "          epochs=config[\"epochs\"],\n",
        "          verbose=1,\n",
        "          # callbacks = [WandbCallback()] #Used with wandb\n",
        "          )\n",
        "    # wandb.finish()\n",
        "    return history"
      ],
      "id": "537a926a"
    },
    {
      "cell_type": "code",
      "source": [
        "history = train()"
      ],
      "metadata": {
        "id": "dTGIG2m6hATI"
      },
      "id": "dTGIG2m6hATI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualization part\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "# plt.savefig('metrics.jpg')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mG9a2xQtg1-u"
      },
      "id": "mG9a2xQtg1-u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5d023074"
      },
      "outputs": [],
      "source": [
        "'''Wandb Sweeps '''\n",
        "# #Sweep configuration for runs\n",
        "# sweep_config = {\n",
        "#   \"name\" : \"best-sweep\",\n",
        "#   \"method\" : \"bayes\",\n",
        "#   \"metric\" : {\n",
        "#       \"name\" : \"val_accuracy\",\n",
        "#       \"goal\" : \"maximize\"\n",
        "#   },\n",
        "#   \"parameters\" : {\n",
        "#     \"epochs\" : {\n",
        "#       \"values\" : [10,20,30]\n",
        "#     },\n",
        "#     \"learning_rate\" :{\n",
        "#       \"values\" : [1e-3,1e-4]\n",
        "#     },\n",
        "#     \"kernel_sizes\":{\n",
        "#         \"values\" : [[(3,3),(3,3),(3,3),(3,3),(3,3)],\n",
        "#                     [(3,3),(3,3),(5,5),(7,7),(7,7)],\n",
        "#                     [(11,11),(11,11),(7,7),(5,5),(3,3)],\n",
        "#                     [(3,3),(5,5),(7,7),(9,9),(11,11)],\n",
        "#                     [(5,5),(5,5),(5,5),(5,5),(5,5)]]\n",
        "#     },\n",
        "#     \"filters_list\":{\n",
        "#         \"values\" : [[32,32,32,32,32],[256,128,64,32,32],[32,64,64,128,128],[32,64,128,256,512],[64,32,64,32,64]]\n",
        "#     },\n",
        "#     \"weight_decay\":{\n",
        "#       \"values\": [0,0.0005,0.005]  \n",
        "#     },\n",
        "#     \"data_augment\":{\n",
        "#         \"values\": [\"True\",\"False\"]\n",
        "#     },\n",
        "#     \"batch_size\":{\n",
        "#         \"values\":[32,64]\n",
        "#     },\n",
        "#     \"activation\":{\n",
        "#         \"values\": [\"relu\",\"elu\",\"swish\",\"gelu\"]\n",
        "#     },\n",
        "#       \"dropout\":{\n",
        "#           \"values\":[0.0,0.2,0.3]\n",
        "#       },\n",
        "#       \"dense_layer_size\":{\n",
        "#           \"values\":[64,128,256,512]\n",
        "#       },\n",
        "#       \"batch_normalization\":{\n",
        "#           \"values\":[\"True\",\"False\"]\n",
        "#       }\n",
        "#   }\n",
        "# }\n",
        "# sweep_id=wandb.sweep(sweep_config,entity=\"dlstack\",project=\"CS6910-ASSIGNMENT-2\")\n",
        "# wandb.agent(sweep_id, function=train, count=20)"
      ],
      "id": "5d023074"
    },
    {
      "cell_type": "code",
      "source": [
        "'''This section is used for loading the models saved with datetime when checkpointing is True'''\n",
        "# #This can be used when checkpointing is set to True and models are saved in model directory with proper name in the current working directory\n",
        "# model_dir = 'models_2022-04-03 00:00:29.823768' #model director name goes here\n",
        "# new_model = tf.keras.models.load_model(model_dir)\n",
        "# # Check its architecture\n",
        "# new_model.summary()"
      ],
      "metadata": {
        "id": "kNQX6n7Ij_mF"
      },
      "id": "kNQX6n7Ij_mF",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cs6910_assignment2_partA_question1_2_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}